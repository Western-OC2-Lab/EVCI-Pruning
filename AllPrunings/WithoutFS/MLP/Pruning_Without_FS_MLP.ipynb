{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e30752e-0395-4b7b-8402-ac2dfebd7299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mnoorche\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import timeit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import keras.models as k_models\n",
    "\n",
    "from scipy.sparse import csr_matrix, save_npz\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import tf_keras as keras\n",
    "from tf_keras import activations\n",
    "from tf_keras.models import Model, Sequential, load_model\n",
    "from tf_keras.layers import Dense, Input, LSTM\n",
    "from tf_keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3658f86-2fd7-485a-a438-dc3e5b283967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50abd01e-7a16-4541-bcb0-37e29b8794ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "mlp_model = k_models.load_model('MLP_HPO.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db1e975a-b8af-47d9-87d3-eca188e28bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_A = pd.read_csv('Final_EVSE_A.csv')\n",
    "df_B = pd.read_csv('Final_EVSE_B.csv')\n",
    "\n",
    "def prepare_categorical_output(y):\n",
    "    # # Print the `y` matrix before encoding\n",
    "    # print(\"y matrix before encoding:\\n\", y)\n",
    "\n",
    "    # Convert Series to NumPy array and reshape `y` matrix\n",
    "    y = y.values.reshape(-1, 1)\n",
    "\n",
    "    # Print the `y` matrix after reshaping\n",
    "    # print(\"y matrix after reshaping:\\n\", y)\n",
    "    \n",
    "    # One-hot encode the target variable\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    y = encoder.fit_transform(y)\n",
    "\n",
    "    # Print the `y` matrix after one-hot encoding\n",
    "    # print(\"y matrix after one-hot encoding:\\n\", y)\n",
    "    \n",
    "    return y\n",
    "\n",
    "#Considering B charging station as training and A as testing\n",
    "\n",
    "def assigning_set(df1, df2):\n",
    "    # Group by 'CSVNameFile' and split the last 20% of each group into the validation set\n",
    "    train_list = []\n",
    "    val_list = []\n",
    "\n",
    "    grouped = df1.groupby('CSVNameFile')\n",
    "\n",
    "    for _, group in grouped:\n",
    "        split_index = int(len(group) * 0.8)\n",
    "        train_list.append(group.iloc[:split_index])\n",
    "        val_list.append(group.iloc[split_index:])\n",
    "\n",
    "    # Concatenate the training and validation sets\n",
    "    train_df = pd.concat(train_list).reset_index(drop=True)\n",
    "    val_df = pd.concat(val_list).reset_index(drop=True)\n",
    "\n",
    "    # Separate features and labels for train and validation sets\n",
    "    X_train = train_df.drop(columns=['CSVNameFile', 'status', 'multiclass'])\n",
    "    y_train = prepare_categorical_output(train_df['multiclass'])\n",
    "\n",
    "    X_val = val_df.drop(columns=['CSVNameFile', 'status', 'multiclass'])\n",
    "    y_val = prepare_categorical_output(val_df['multiclass'])\n",
    "\n",
    "    # X_test and y_test from df2 remain unchanged for test evaluation\n",
    "    X_test = df2.drop(columns=['CSVNameFile', 'status', 'multiclass'])\n",
    "    y_test = prepare_categorical_output(df2['multiclass'])\n",
    "\n",
    "    input_dim = X_train.shape[1]\n",
    "    output_dim = len(np.unique(df1['multiclass']))\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, input_dim, output_dim\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, input_dim, output_dim = assigning_set(df_B, df_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fed4666-198e-4ea5-90e9-c9d65988e835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">800</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,176</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                  │             \u001b[38;5;34m800\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │           \u001b[38;5;34m2,176\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m8,256\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                   │             \u001b[38;5;34m195\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,283</span> (133.92 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m34,283\u001b[0m (133.92 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,427</span> (44.64 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m11,427\u001b[0m (44.64 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">22,856</span> (89.29 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m22,856\u001b[0m (89.29 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1473df00-7650-4b87-b2eb-646393fa9e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Dimension: 49\n",
      "Output Dimension: 3\n",
      "WARNING:tensorflow:From C:\\Users\\mnoorche\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\backend.py:1400: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\mnoorche\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\optimizers\\__init__.py:317: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:From C:\\Users\\mnoorche\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\mnoorche\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "54921/54921 [==============================] - 52s 937us/step - loss: 1691.2175 - accuracy: 0.8793 - val_loss: 0.1635 - val_accuracy: 0.9336\n",
      "Epoch 2/50\n",
      "54921/54921 [==============================] - 52s 942us/step - loss: 12.0309 - accuracy: 0.9296 - val_loss: 0.1686 - val_accuracy: 0.9291\n",
      "Epoch 3/50\n",
      "54921/54921 [==============================] - 52s 942us/step - loss: 15.6137 - accuracy: 0.9268 - val_loss: 0.1677 - val_accuracy: 0.9343\n",
      "Epoch 4/50\n",
      "54921/54921 [==============================] - 52s 948us/step - loss: 5.1026 - accuracy: 0.9263 - val_loss: 0.1577 - val_accuracy: 0.9366\n",
      "Epoch 5/50\n",
      "54921/54921 [==============================] - 52s 945us/step - loss: 9.3021 - accuracy: 0.9299 - val_loss: 0.1781 - val_accuracy: 0.9319\n",
      "Epoch 6/50\n",
      "54921/54921 [==============================] - 53s 956us/step - loss: 0.1826 - accuracy: 0.9303 - val_loss: 0.2227 - val_accuracy: 0.9165\n",
      "Epoch 7/50\n",
      "54921/54921 [==============================] - 52s 941us/step - loss: 131.8143 - accuracy: 0.9308 - val_loss: 0.1660 - val_accuracy: 0.9182\n",
      "Epoch 8/50\n",
      "54921/54921 [==============================] - 52s 945us/step - loss: 0.1656 - accuracy: 0.9311 - val_loss: 0.1490 - val_accuracy: 0.9366\n",
      "Epoch 9/50\n",
      "54921/54921 [==============================] - 52s 940us/step - loss: 0.1701 - accuracy: 0.9313 - val_loss: 0.1479 - val_accuracy: 0.9363\n",
      "Epoch 10/50\n",
      "54921/54921 [==============================] - 51s 921us/step - loss: 0.1594 - accuracy: 0.9313 - val_loss: 0.1535 - val_accuracy: 0.9229\n",
      "Epoch 11/50\n",
      "54921/54921 [==============================] - 52s 954us/step - loss: 0.1606 - accuracy: 0.9311 - val_loss: 0.1452 - val_accuracy: 0.9362\n",
      "Epoch 12/50\n",
      "54921/54921 [==============================] - 52s 938us/step - loss: 0.2028 - accuracy: 0.9305 - val_loss: 0.1462 - val_accuracy: 0.9366\n",
      "Epoch 13/50\n",
      "54921/54921 [==============================] - 51s 929us/step - loss: 0.1592 - accuracy: 0.9310 - val_loss: 0.1470 - val_accuracy: 0.9339\n",
      "Epoch 14/50\n",
      "54921/54921 [==============================] - 51s 925us/step - loss: 0.1593 - accuracy: 0.9312 - val_loss: 0.1615 - val_accuracy: 0.9174\n",
      "Epoch 15/50\n",
      "54921/54921 [==============================] - 52s 945us/step - loss: 91.9796 - accuracy: 0.9310 - val_loss: 0.1577 - val_accuracy: 0.9182\n",
      "Epoch 16/50\n",
      "54921/54921 [==============================] - 52s 947us/step - loss: 2.9823 - accuracy: 0.9308 - val_loss: 0.1455 - val_accuracy: 0.9364\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 16)                800       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               2176      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11427 (44.64 KB)\n",
      "Trainable params: 11427 (44.64 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mnoorche\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Recreating the model into compatibale sequential format, using the same architecture\n",
    "\n",
    "# Get input and output shapes of the model\n",
    "input_dim = mlp_model.input_shape[1]  # Get input dimension \n",
    "output_dim = mlp_model.output_shape[1]  # Get output dimension\n",
    "\n",
    "print(\"Input Dimension:\", input_dim)\n",
    "print(\"Output Dimension:\", output_dim)\n",
    "\n",
    "# Recreate the model in Sequential format\n",
    "new_mlp_model = Sequential([\n",
    "    Input(shape=(input_dim,)), \n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(128, activation='relu'),  \n",
    "    Dense(64, activation='relu'),   \n",
    "    Dense(output_dim, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the new model with the same settings\n",
    "new_mlp_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the new model\n",
    "new_mlp_model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                   callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)],\n",
    "                   batch_size=32, epochs=50, verbose=1)\n",
    "\n",
    "\n",
    "new_mlp_model.save('MLP.h5')\n",
    "\n",
    "# Display the new model summary to confirm the architectur\n",
    "new_mlp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "320615fb-8e82-4f50-aca4-4de8e1c991f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17121/17121 [==============================] - 14s 841us/step\n",
      "Test Accuracy: 0.9841\n",
      "Test Precision: 0.9858\n",
      "Test Recall: 0.9841\n",
      "Test F1 Score: 0.9846\n"
     ]
    }
   ],
   "source": [
    "y_pred = new_mlp_model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_test_classes, y_pred_classes)\n",
    "precision = precision_score(y_test_classes, y_pred_classes, average='weighted')\n",
    "recall = recall_score(y_test_classes, y_pred_classes, average='weighted')\n",
    "f1 = f1_score(y_test_classes, y_pred_classes, average='weighted')\n",
    "\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n",
    "print(f'Test Precision: {precision:.4f}')\n",
    "print(f'Test Recall: {recall:.4f}')\n",
    "print(f'Test F1 Score: {f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "c2259e63-486a-4df3-9795-9dcaf1218e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved weights for layer dense to before_pruning_MLP_dense_weights.csv\n",
      "Saved biases for layer dense to before_pruning_MLP_dense_biases.csv\n",
      "Saved weights for layer dense_1 to before_pruning_MLP_dense_1_weights.csv\n",
      "Saved biases for layer dense_1 to before_pruning_MLP_dense_1_biases.csv\n",
      "Saved weights for layer dense_2 to before_pruning_MLP_dense_2_weights.csv\n",
      "Saved biases for layer dense_2 to before_pruning_MLP_dense_2_biases.csv\n",
      "Saved weights for layer dense_3 to before_pruning_MLP_dense_3_weights.csv\n",
      "Saved biases for layer dense_3 to before_pruning_MLP_dense_3_biases.csv\n"
     ]
    }
   ],
   "source": [
    "# Function to save weights to CSV\n",
    "def save_weights_to_csv(model, file_prefix):\n",
    "    for layer in model.layers:\n",
    "        weights = layer.get_weights()  # This returns a list of NumPy arrays (weights and biases)\n",
    "        for i, weight in enumerate(weights):\n",
    "            layer_name = layer.name\n",
    "            weight_type = 'weights' if i == 0 else 'biases'  # Name based on weights or biases\n",
    "            file_name = f\"{file_prefix}_{layer_name}_{weight_type}.csv\"\n",
    "            \n",
    "            # Save the weights or biases to CSV\n",
    "            pd.DataFrame(weight).to_csv(file_name, index=False)\n",
    "            print(f\"Saved {weight_type} for layer {layer_name} to {file_name}\")\n",
    "\n",
    "# Save weights for the original model\n",
    "save_weights_to_csv(new_mlp_model, \"before_pruning_MLP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "4130bce6-348c-418a-ae5f-5ca54a6fa495",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pruning(model, X_train, X_val, X_test, y_train, y_val, y_test, final_sparsity, df_name):\n",
    "    \n",
    "    # dataset_size = the number of samples in the training set.\n",
    "    # batch_size = the number of samples processed in one training step.\n",
    "    # num_epochs = the number of times the entire training set is used to update the model.\n",
    "    \n",
    "    # Parameters for the dataset and training\n",
    "    dataset_size = len(X_train)\n",
    "    batch_size = 32  \n",
    "    num_epochs = 20\n",
    "    \n",
    "    # Pruning parameters as percentages of the total steps\n",
    "    start_pct = 0  # Start pruning at the beginning of the training steps\n",
    "    end_pct = 0.4    # End pruning after 40% of the training steps\n",
    "\n",
    "    # Step calculations\n",
    "    steps_per_epoch = dataset_size / batch_size\n",
    "    total_steps = steps_per_epoch * num_epochs\n",
    "\n",
    "    start_step = int(total_steps * start_pct)\n",
    "    end_step = int(total_steps * end_pct)\n",
    "\n",
    "    # Display the calculated steps\n",
    "    print(f\"Total Steps: {total_steps}\")\n",
    "    print(f\"Start Step: {start_step}\")\n",
    "    print(f\"End Step: {end_step}\")\n",
    "    \n",
    "    start_epoch = start_step // steps_per_epoch\n",
    "    end_epoch = end_step // steps_per_epoch\n",
    "    print(f\"Pruning will start in epoch {int(start_epoch)} and end in epoch {int(end_epoch)}\")\n",
    "\n",
    "    \n",
    "    pruning_params = {\n",
    "        'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.60,\n",
    "                                                                 final_sparsity=final_sparsity,\n",
    "                                                                 begin_step=start_step,\n",
    "                                                                 end_step=end_step,\n",
    "                                                                 power=1,\n",
    "                                                                 frequency=100),\n",
    "    }\n",
    "\n",
    "    pruned_model = tfmot.sparsity.keras.prune_low_magnitude(\n",
    "        model, **pruning_params\n",
    "    )   \n",
    "\n",
    "    \n",
    "    # Compile the pruned model\n",
    "    pruned_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    # Set up pruning callbacks and early stopping\n",
    "    pruning_callbacks = [\n",
    "        tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "        tfmot.sparsity.keras.PruningSummaries(log_dir=tempfile.mkdtemp())\n",
    "    ]\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "      \n",
    "    callbacks = pruning_callbacks + [early_stopping]\n",
    "\n",
    "    history = pruned_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=num_epochs, batch_size=batch_size, callbacks=callbacks)\n",
    "\n",
    "    # Strip pruning wrappers\n",
    "    final_model = tfmot.sparsity.keras.strip_pruning(pruned_model)\n",
    "\n",
    "    # Compile the model again after stripping the pruning wrappers\n",
    "    final_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    final_model.save(f'{df_name}.h5') \n",
    "    \n",
    "    # The number of epochs should be greater than num_epochs_for_end_step to make sure pruning is complete\n",
    "    num_epochs_for_end_step = int(end_step // steps_per_epoch)\n",
    "    the_number_of_epochs = len(history.history['loss'])\n",
    "    # Print the number of epochs and the epoch when pruning finished\n",
    "    print(f\"Number of epochs is: {the_number_of_epochs}, and pruning finished at epoch {num_epochs_for_end_step}\")\n",
    "    \n",
    "    # Check if pruning completed before early stopping\n",
    "    pruning_completed = the_number_of_epochs > num_epochs_for_end_step\n",
    "    print(f\"Pruning completed before early stopping: {pruning_completed}\")\n",
    "\n",
    "    y_pred = final_model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_test_classes, y_pred_classes)\n",
    "    precision = precision_score(y_test_classes, y_pred_classes, average='weighted')\n",
    "    recall = recall_score(y_test_classes, y_pred_classes, average='weighted')\n",
    "    f1 = f1_score(y_test_classes, y_pred_classes, average='weighted')\n",
    "\n",
    "    print(f'Test Accuracy: {accuracy:.4f}')\n",
    "    print(f'Test Precision: {precision:.4f}')\n",
    "    print(f'Test Recall: {recall:.4f}')\n",
    "    print(f'Test F1 Score: {f1:.4f}')\n",
    "\n",
    "    # Save weights for the pruned model\n",
    "    save_weights_to_csv(final_model, f'after_pruning_{df_name}')\n",
    "    \n",
    "    return final_model, pruning_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "c5d803be-5d58-4a5d-8075-49645a93df1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Steps: 1098415.0\n",
      "Start Step: 0\n",
      "End Step: 439366\n",
      "Pruning will start in epoch 0 and end in epoch 8\n",
      "Epoch 1/20\n",
      "    1/54921 [..............................] - ETA: 38:19:13 - loss: 0.2477 - accuracy: 0.9688WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_train_batch_end` time: 0.0512s). Check your callbacks.\n",
      "54921/54921 [==============================] - 96s 2ms/step - loss: 197.1284 - accuracy: 0.9176 - val_loss: 0.1479 - val_accuracy: 0.9163\n",
      "Epoch 2/20\n",
      "54921/54921 [==============================] - 93s 2ms/step - loss: 1.6267 - accuracy: 0.9313 - val_loss: 0.1874 - val_accuracy: 0.9080\n",
      "Epoch 3/20\n",
      "54921/54921 [==============================] - 93s 2ms/step - loss: 639.7868 - accuracy: 0.9274 - val_loss: 0.1722 - val_accuracy: 0.9276\n",
      "Epoch 4/20\n",
      "54921/54921 [==============================] - 93s 2ms/step - loss: 6.6030 - accuracy: 0.9314 - val_loss: 0.1543 - val_accuracy: 0.9364\n",
      "Epoch 5/20\n",
      "54921/54921 [==============================] - 93s 2ms/step - loss: 6.4820 - accuracy: 0.9343 - val_loss: 0.1434 - val_accuracy: 0.9362\n",
      "Epoch 6/20\n",
      "54921/54921 [==============================] - 93s 2ms/step - loss: 113.2824 - accuracy: 0.9346 - val_loss: 0.1435 - val_accuracy: 0.9364\n",
      "Epoch 7/20\n",
      "54921/54921 [==============================] - 93s 2ms/step - loss: 622.1073 - accuracy: 0.9347 - val_loss: 0.1422 - val_accuracy: 0.9365\n",
      "Epoch 8/20\n",
      "54921/54921 [==============================] - 93s 2ms/step - loss: 368.9074 - accuracy: 0.9348 - val_loss: 0.1437 - val_accuracy: 0.9356\n",
      "Epoch 9/20\n",
      "54921/54921 [==============================] - 93s 2ms/step - loss: 960.5702 - accuracy: 0.9345 - val_loss: 0.1426 - val_accuracy: 0.9362\n",
      "Epoch 10/20\n",
      "54921/54921 [==============================] - 93s 2ms/step - loss: 115.9558 - accuracy: 0.9350 - val_loss: 0.1418 - val_accuracy: 0.9362\n",
      "Epoch 11/20\n",
      "54921/54921 [==============================] - 94s 2ms/step - loss: 0.1475 - accuracy: 0.9351 - val_loss: 0.1417 - val_accuracy: 0.9362\n",
      "Epoch 12/20\n",
      "54921/54921 [==============================] - 93s 2ms/step - loss: 0.1476 - accuracy: 0.9352 - val_loss: 0.1407 - val_accuracy: 0.9363\n",
      "Epoch 13/20\n",
      "54921/54921 [==============================] - 94s 2ms/step - loss: 0.1438 - accuracy: 0.9352 - val_loss: 0.1404 - val_accuracy: 0.9364\n",
      "Epoch 14/20\n",
      "54921/54921 [==============================] - 93s 2ms/step - loss: 0.1440 - accuracy: 0.9351 - val_loss: 0.1416 - val_accuracy: 0.9364\n",
      "Epoch 15/20\n",
      "54921/54921 [==============================] - 94s 2ms/step - loss: 225.9558 - accuracy: 0.9348 - val_loss: 0.1413 - val_accuracy: 0.9361\n",
      "Epoch 16/20\n",
      "54921/54921 [==============================] - 93s 2ms/step - loss: 92.4818 - accuracy: 0.9348 - val_loss: 0.1439 - val_accuracy: 0.9361\n",
      "Epoch 17/20\n",
      "54921/54921 [==============================] - 93s 2ms/step - loss: 36.0410 - accuracy: 0.9350 - val_loss: 0.1447 - val_accuracy: 0.9361\n",
      "Epoch 18/20\n",
      "54921/54921 [==============================] - 94s 2ms/step - loss: 1753.0358 - accuracy: 0.9349 - val_loss: 0.1426 - val_accuracy: 0.9363\n",
      "Number of epochs is: 18, and pruning finished at epoch 8\n",
      "Pruning completed before early stopping: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mnoorche\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17121/17121 [==============================] - 14s 837us/step\n",
      "Test Accuracy: 0.9804\n",
      "Test Precision: 0.9854\n",
      "Test Recall: 0.9804\n",
      "Test F1 Score: 0.9817\n",
      "Saved weights for layer dense to after_pruning_PrunedMLP_dense_weights.csv\n",
      "Saved biases for layer dense to after_pruning_PrunedMLP_dense_biases.csv\n",
      "Saved weights for layer dense_1 to after_pruning_PrunedMLP_dense_1_weights.csv\n",
      "Saved biases for layer dense_1 to after_pruning_PrunedMLP_dense_1_biases.csv\n",
      "Saved weights for layer dense_2 to after_pruning_PrunedMLP_dense_2_weights.csv\n",
      "Saved biases for layer dense_2 to after_pruning_PrunedMLP_dense_2_biases.csv\n",
      "Saved weights for layer dense_3 to after_pruning_PrunedMLP_dense_3_weights.csv\n",
      "Saved biases for layer dense_3 to after_pruning_PrunedMLP_dense_3_biases.csv\n"
     ]
    }
   ],
   "source": [
    "the_mlp_model = load_model('MLP.h5')\n",
    "PrunedMLP, pruning_completed = pruning(the_mlp_model, X_train, X_val, X_test, y_train, y_val, y_test, 0.65, 'PrunedMLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "95289931-f60a-498e-835d-f14320f8214c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing weights for layer: dense\n",
      "Weights in layer 'dense':\n",
      "  Total number of weights: 784\n",
      "  Weights changed to zero during pruning: 510\n",
      "  Percentage of zero weights after pruning: 65.05%\n",
      "\n",
      "Biases in layer 'dense':\n",
      "  Total number of weights: 16\n",
      "  Weights changed to zero during pruning: 0\n",
      "  Percentage of zero weights after pruning: 0.00%\n",
      "\n",
      "Comparing weights for layer: dense_1\n",
      "Weights in layer 'dense_1':\n",
      "  Total number of weights: 2048\n",
      "  Weights changed to zero during pruning: 1331\n",
      "  Percentage of zero weights after pruning: 64.99%\n",
      "\n",
      "Biases in layer 'dense_1':\n",
      "  Total number of weights: 128\n",
      "  Weights changed to zero during pruning: 0\n",
      "  Percentage of zero weights after pruning: 0.00%\n",
      "\n",
      "Comparing weights for layer: dense_2\n",
      "Weights in layer 'dense_2':\n",
      "  Total number of weights: 8192\n",
      "  Weights changed to zero during pruning: 5325\n",
      "  Percentage of zero weights after pruning: 65.00%\n",
      "\n",
      "Biases in layer 'dense_2':\n",
      "  Total number of weights: 64\n",
      "  Weights changed to zero during pruning: 0\n",
      "  Percentage of zero weights after pruning: 0.00%\n",
      "\n",
      "Comparing weights for layer: dense_3\n",
      "Weights in layer 'dense_3':\n",
      "  Total number of weights: 192\n",
      "  Weights changed to zero during pruning: 125\n",
      "  Percentage of zero weights after pruning: 65.10%\n",
      "\n",
      "Biases in layer 'dense_3':\n",
      "  Total number of weights: 3\n",
      "  Weights changed to zero during pruning: 0\n",
      "  Percentage of zero weights after pruning: 0.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to compare weights before and after pruning, and calculate percentage of zero weights\n",
    "def compare_weights_before_after(model_before, model_after):\n",
    "    for layer in model_before.layers:\n",
    "        # Get the layer name\n",
    "        layer_name = layer.name\n",
    "        \n",
    "        # Get weights and biases from both models\n",
    "        weights_before = model_before.get_layer(layer_name).get_weights()\n",
    "        weights_after = model_after.get_layer(layer_name).get_weights()\n",
    "        \n",
    "        print(f\"Comparing weights for layer: {layer_name}\")\n",
    "        \n",
    "        for i, (before, after) in enumerate(zip(weights_before, weights_after)):\n",
    "            weight_type = 'weights' if i == 0 else 'biases'\n",
    "            \n",
    "            # Calculate the total number of weights\n",
    "            total_weights = np.prod(before.shape)\n",
    "            \n",
    "            # Calculate the number of weights that have changed to zero\n",
    "            changes_to_zero = (before != 0) & (after == 0)\n",
    "            num_changes_to_zero = np.sum(changes_to_zero)\n",
    "            \n",
    "            # Calculate the number of zero weights after pruning\n",
    "            num_zero_weights_after = np.sum(after == 0)\n",
    "            \n",
    "            # Calculate the percentage of weights that are now zero\n",
    "            percentage_zero_weights = (num_zero_weights_after / total_weights) * 100\n",
    "            \n",
    "            # Print the results\n",
    "            print(f\"{weight_type.capitalize()} in layer '{layer_name}':\")\n",
    "            print(f\"  Total number of weights: {total_weights}\")\n",
    "            print(f\"  Weights changed to zero during pruning: {num_changes_to_zero}\")\n",
    "            print(f\"  Percentage of zero weights after pruning: {percentage_zero_weights:.2f}%\")\n",
    "            print(\"\")\n",
    "\n",
    "# Call the function to compare the original model and the pruned model\n",
    "compare_weights_before_after(new_mlp_model, PrunedMLP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "c9db8e2a-4e8f-47dc-9268-2d4adcd8b2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducing size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "7c18b95e-dbc0-4d29-b170-9213953956ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: dense, Weight dtype: float32, Bias dtype: float32\n",
      "Layer: dense_1, Weight dtype: float32, Bias dtype: float32\n",
      "Layer: dense_2, Weight dtype: float32, Bias dtype: float32\n",
      "Layer: dense_3, Weight dtype: float32, Bias dtype: float32\n"
     ]
    }
   ],
   "source": [
    "for layer in PrunedMLP.layers:\n",
    "    weights = layer.get_weights()\n",
    "    if len(weights) > 0:\n",
    "        print(f\"Layer: {layer.name}, Weight dtype: {weights[0].dtype}, Bias dtype: {weights[1].dtype}\" if len(weights) > 1 else f\"Layer: {layer.name}, Weight dtype: {weights[0].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "5300ca6e-1693-49ab-b6d4-297fe952fca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform inference with sparse matrix multiplication while keeping everything sparse\n",
    "def sparse_inference(model, input_data):\n",
    "    layer_outputs = input_data  # Start with input data (dense)\n",
    "\n",
    "    # start_time = timeit.default_timer()  # Start timing the inference\n",
    "\n",
    "    for layer in model.layers:\n",
    "        if 'dense' in layer.name:  # Only focus on Dense layers\n",
    "            dense_weights = layer.get_weights()[0]  # Get dense weights\n",
    "            bias = layer.get_weights()[1]  # Get biases\n",
    "\n",
    "            # print(f\"Layer: {layer.name}, Weight shape: {dense_weights.shape}, Bias shape: {bias.shape}\")\n",
    "            # print(f\"Input shape before multiplication: {layer_outputs.shape}\")\n",
    "\n",
    "            # Convert dense weights to sparse matrix\n",
    "            sparse_weights = csr_matrix(dense_weights)\n",
    "\n",
    "            # Perform matrix multiplication\n",
    "            layer_outputs = layer_outputs @ sparse_weights  # Matrix multiplication\n",
    "            # print(f\"Output shape after multiplication: {layer_outputs.shape}\")\n",
    "\n",
    "            # Add bias to each output row\n",
    "            layer_outputs = layer_outputs + bias\n",
    "            # print(f\"Output shape after adding bias: {layer_outputs.shape}\")\n",
    "\n",
    "            # Apply the activation function to the non-zero values in the sparse matrix\n",
    "            if layer.activation is not None:\n",
    "                activation_function = layer.activation  # Get the activation function\n",
    "                \n",
    "                # Manually apply relu or softmax to sparse matrix\n",
    "                if activation_function == activations.relu:\n",
    "                    layer_outputs.data = np.maximum(0, layer_outputs.data)  # Apply ReLU to non-zero values\n",
    "\n",
    "                elif activation_function == activations.softmax:\n",
    "                    # Apply softmax directly on 1D output (we are at the final output layer)\n",
    "                    if layer_outputs.shape[1] == 3:  # If we are at the final layer with 3 outputs\n",
    "                        exp_values = np.exp(layer_outputs - np.max(layer_outputs))\n",
    "                        softmax_outputs = exp_values / np.sum(exp_values, axis=1)\n",
    "                        layer_outputs = softmax_outputs\n",
    "                        # print(f\"Softmax applied: {softmax_outputs}\")\n",
    "                else:\n",
    "                    # For other activations, raise an error\n",
    "                    raise NotImplementedError(f\"Activation {activation_function} is not implemented for sparse matrices\")\n",
    "\n",
    "                # print(f\"Output shape after applying activation to non-zero values: {layer_outputs.shape}\")\n",
    "\n",
    "    # end_time = timeit.default_timer()  # Stop timing the inference\n",
    "\n",
    "    # inference_time = end_time - start_time  # Calculate total inference time\n",
    "    # print(f\"Total inference time: {inference_time * 1000:.6f} milliseconds\")  # Print time in milliseconds\n",
    "\n",
    "    return layer_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "3054c91b-5104-4f0b-a47d-09a563e19508",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mnoorche\\AppData\\Local\\Temp\\ipykernel_1244\\2912187159.py:32: DeprecationWarning: Assigning the 'data' attribute is an inherently unsafe operation and will be removed in the future.\n",
      "  layer_outputs.data = np.maximum(0, layer_outputs.data)  # Apply ReLU to non-zero values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction of the PrunedMLP using sparse_inference function: [[3.04889482e-006 1.97502139e-119 9.99996951e-001]]\n",
      "1/1 [==============================] - 1s 569ms/step\n",
      "Prediction of the PrunedMLP using predict method: [[3.0488914e-06 0.0000000e+00 9.9999690e-01]]\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Prediction of the original MLP using predict method: [[8.812600e-03 1.119682e-12 9.911874e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Example input for inference\n",
    "input_data = np.random.rand(1, 49) \n",
    "\n",
    "\n",
    "# Perform inference using sparse matrix multiplication\n",
    "sparse_inference_output = sparse_inference(PrunedMLP, input_data)\n",
    "print('Prediction of the PrunedMLP using sparse_inference function:', sparse_inference_output)\n",
    "\n",
    "predict_pruned_inference_output = PrunedMLP.predict(input_data)\n",
    "print('Prediction of the PrunedMLP using predict method:', predict_pruned_inference_output)\n",
    "\n",
    "predict_original_inference_output = new_mlp_model.predict(input_data)\n",
    "print('Prediction of the original MLP using predict method:', predict_original_inference_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "f5872c9b-f061-45d3-b36c-32eefbc58048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_inference_time(model, pruned_model, X_test):\n",
    "    # Lists to store inference times for both models\n",
    "    model_time_list = []\n",
    "    pruned_model_time_list = []\n",
    "\n",
    "    # Model before pruning (original model)\n",
    "    for sample in X_test:\n",
    "        start_time = timeit.default_timer()\n",
    "        model.predict(np.expand_dims(sample, axis=0), verbose=False)\n",
    "        end_time = timeit.default_timer()\n",
    "        model_time_list.append((end_time - start_time) * 1000)  # Convert to milliseconds\n",
    "    \n",
    "    avg_inference_time_model = np.mean(model_time_list)\n",
    "    print(f\"Average inference time per sample for Model before pruning: {avg_inference_time_model:.6f} milliseconds\")\n",
    "\n",
    "    # Model after pruning (pruned model)\n",
    "    for sample in X_test:\n",
    "        start_time = timeit.default_timer()\n",
    "        sparse_inference(pruned_model, np.expand_dims(sample, axis=0))\n",
    "        end_time = timeit.default_timer()\n",
    "        pruned_model_time_list.append((end_time - start_time) * 1000)  # Convert to milliseconds\n",
    "    \n",
    "    avg_inference_time_pruned_model = np.mean(pruned_model_time_list)\n",
    "    print(f\"Average inference time per sample for Model after pruning: {avg_inference_time_pruned_model:.6f} milliseconds\")\n",
    "   \n",
    "    # Save the lists as NumPy arrays\n",
    "    np.save('model_time_list.npy', np.array(model_time_list))\n",
    "    np.save('pruned_model_time_list.npy', np.array(pruned_model_time_list))\n",
    "    \n",
    "    return avg_inference_time_model, avg_inference_time_pruned_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "9e01664b-2fe2-4f8f-afaf-b24803adc37e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(547854, 49)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "cfc3ea1a-99f3-4b02-927d-330104e6b552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsampled X_test shape: (54785, 49)\n"
     ]
    }
   ],
   "source": [
    "X_test_subsample = X_test.sample(frac=0.1, random_state=42)\n",
    "\n",
    "print(f\"Subsampled X_test shape: {X_test_subsample.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "e502c94c-0eee-4567-bb97-7a9404941c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per sample for Model before pruning: 63.704041 milliseconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mnoorche\\AppData\\Local\\Temp\\ipykernel_1244\\2912187159.py:32: DeprecationWarning: Assigning the 'data' attribute is an inherently unsafe operation and will be removed in the future.\n",
      "  layer_outputs.data = np.maximum(0, layer_outputs.data)  # Apply ReLU to non-zero values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per sample for Model after pruning: 3.016179 milliseconds\n"
     ]
    }
   ],
   "source": [
    "# Convert DataFrame to NumPy array\n",
    "X_test_subsample_numpy = X_test_subsample.to_numpy(dtype=np.float32)\n",
    "avg_time_model, avg_time_pruned_model = measure_inference_time(new_mlp_model, PrunedMLP, X_test_subsample_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e11ee83-71a3-47ee-830c-35e1f155e98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved original model weights to original_model_weights.npz\n",
      "Saved pruned model weights to pruned_model_weights.npz\n",
      "Size of original_model_weights.npz: 46.70 KB\n",
      "Size of pruned_model_weights.npz: 36.46 KB\n"
     ]
    }
   ],
   "source": [
    "# Function to save all weights of the original model in a single .npz file\n",
    "def save_original_weights_as_npz(model, file_name):\n",
    "    all_weights = {}\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        weights = layer.get_weights()\n",
    "        if weights:\n",
    "            all_weights[f\"layer_{i}_weights\"] = weights[0]  # Save weights\n",
    "            all_weights[f\"layer_{i}_biases\"] = weights[1]  # Save biases\n",
    "\n",
    "    np.savez(file_name, **all_weights)\n",
    "    print(f\"Saved original model weights to {file_name}.npz\")\n",
    "\n",
    "# Function to save all pruned (sparse) weights in a single .npz file\n",
    "def save_pruned_weights_as_npz(model, file_name):\n",
    "    all_sparse_weights = {}\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        weights = layer.get_weights()\n",
    "        if weights:\n",
    "            sparse_weight = csr_matrix(weights[0])  # Convert dense weights to sparse\n",
    "            all_sparse_weights[f\"layer_{i}_sparse_weights\"] = sparse_weight\n",
    "            all_sparse_weights[f\"layer_{i}_biases\"] = weights[1]  # Save biases as normal\n",
    "\n",
    "    # Save the sparse weights in .npz format\n",
    "    np.savez(file_name, **all_sparse_weights)\n",
    "    print(f\"Saved pruned model weights to {file_name}.npz\")\n",
    "\n",
    "# Save all weights for original and pruned models\n",
    "save_original_weights_as_npz(new_mlp_model, 'original_model_weights')\n",
    "save_pruned_weights_as_npz(PrunedMLP, 'pruned_model_weights')\n",
    "\n",
    "\n",
    "# Function to compare the sizes of two files\n",
    "def compare_file_sizes(file1, file2):\n",
    "    size1 = os.path.getsize(file1) / 1024  # Convert to KB\n",
    "    size2 = os.path.getsize(file2) / 1024  # Convert to KB\n",
    "    print(f\"Size of {file1}: {size1:.2f} KB\")\n",
    "    print(f\"Size of {file2}: {size2:.2f} KB\")\n",
    "\n",
    "# Comparing the size of the original and pruned model weight files\n",
    "compare_file_sizes('original_model_weights.npz', 'pruned_model_weights.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "c623e70f-1f8f-4ddf-9e62-843e3693e746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loading the weights from a saved .npz file\n",
    "# def load_weights_from_npz(file_name):\n",
    "#     loaded = np.load(file_name)\n",
    "#     print(f\"Loaded weights from {file_name}\")\n",
    "#     return loaded\n",
    "\n",
    "# # Load the saved weights for both models\n",
    "# original_weights = load_weights_from_npz('original_model_weights.npz')\n",
    "# pruned_weights = load_weights_from_npz('pruned_model_weights.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64acad35-b1dd-489a-874e-075c188b3378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_mlp_model = load_model('MLP.h5')\n",
    "# PrunedMLP = load_model('PrunedMLP.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a48a50-015d-4594-8fc1-29e55f69fe7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
