{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e30752e-0395-4b7b-8402-ac2dfebd7299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mnoorche\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import timeit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import keras.models as k_models\n",
    "\n",
    "from scipy.sparse import csr_matrix, save_npz\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import tf_keras as keras\n",
    "from tf_keras import activations\n",
    "from tf_keras.models import Model, Sequential, load_model\n",
    "from tf_keras.layers import Dense, Input, LSTM\n",
    "from tf_keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3658f86-2fd7-485a-a438-dc3e5b283967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50abd01e-7a16-4541-bcb0-37e29b8794ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "lstm_model = k_models.load_model('LSTM_HPO.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db1e975a-b8af-47d9-87d3-eca188e28bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_A = pd.read_csv('Final_EVSE_A.csv')\n",
    "df_B = pd.read_csv('Final_EVSE_B.csv')\n",
    "\n",
    "def prepare_categorical_output(y):\n",
    "    # # Print the `y` matrix before encoding\n",
    "    # print(\"y matrix before encoding:\\n\", y)\n",
    "\n",
    "    # Convert Series to NumPy array and reshape `y` matrix\n",
    "    y = y.values.reshape(-1, 1)\n",
    "\n",
    "    # Print the `y` matrix after reshaping\n",
    "    # print(\"y matrix after reshaping:\\n\", y)\n",
    "    \n",
    "    # One-hot encode the target variable\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    y = encoder.fit_transform(y)\n",
    "\n",
    "    # Print the `y` matrix after one-hot encoding\n",
    "    # print(\"y matrix after one-hot encoding:\\n\", y)\n",
    "    \n",
    "    return y\n",
    "\n",
    "#Considering B charging station as training and A as testing\n",
    "\n",
    "def assigning_set(df1, df2):\n",
    "    # Group by 'CSVNameFile' and split the last 20% of each group into the validation set\n",
    "    train_list = []\n",
    "    val_list = []\n",
    "\n",
    "    grouped = df1.groupby('CSVNameFile')\n",
    "\n",
    "    for _, group in grouped:\n",
    "        split_index = int(len(group) * 0.8)\n",
    "        train_list.append(group.iloc[:split_index])\n",
    "        val_list.append(group.iloc[split_index:])\n",
    "\n",
    "    # Concatenate the training and validation sets\n",
    "    train_df = pd.concat(train_list).reset_index(drop=True)\n",
    "    val_df = pd.concat(val_list).reset_index(drop=True)\n",
    "\n",
    "    # Separate features and labels for train and validation sets\n",
    "    X_train = train_df.drop(columns=['CSVNameFile', 'status', 'multiclass'])\n",
    "    y_train = prepare_categorical_output(train_df['multiclass'])\n",
    "\n",
    "    X_val = val_df.drop(columns=['CSVNameFile', 'status', 'multiclass'])\n",
    "    y_val = prepare_categorical_output(val_df['multiclass'])\n",
    "\n",
    "    # X_test and y_test from df2 remain unchanged for test evaluation\n",
    "    X_test = df2.drop(columns=['CSVNameFile', 'status', 'multiclass'])\n",
    "    y_test = prepare_categorical_output(df2['multiclass'])\n",
    "\n",
    "    input_dim = X_train.shape[1]\n",
    "    output_dim = len(np.unique(df1['multiclass']))\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, input_dim, output_dim\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, input_dim, output_dim = assigning_set(df_B, df_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c43348ef-5e22-4062-b93c-dcade5c96db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1757464, 49) (1757464, 3)\n",
      "(439382, 49) (439382, 3)\n",
      "(547854, 49) (547854, 3)\n",
      "After using create_sequence\n",
      "(1757459, 5, 49) (1757459, 3)\n",
      "(439377, 5, 49) (439377, 3)\n",
      "(547849, 5, 49) (547849, 3)\n"
     ]
    }
   ],
   "source": [
    "def create_sequences(X, y, timesteps):\n",
    "    X_seq, y_seq = [], []\n",
    "    X = X.values  # Convert DataFrame to Numpy array\n",
    "    for i in range(len(X) - timesteps):\n",
    "        X_seq.append(X[i:i + timesteps])\n",
    "        y_seq.append(y[i + timesteps - 1])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# Generate sequences for training, validation, and testing using the sliding window approach\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "timesteps = 5\n",
    "\n",
    "X_train, y_train = create_sequences(X_train, y_train, timesteps)\n",
    "X_val, y_val = create_sequences(X_val, y_val, timesteps)\n",
    "X_test, y_test = create_sequences(X_test, y_test, timesteps)\n",
    "\n",
    "print('After using create_sequence')\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fed4666-198e-4ea5-90e9-c9d65988e835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)              │          <span style=\"color: #00af00; text-decoration-color: #00af00\">60,000</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">30,200</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">153</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m100\u001b[0m)              │          \u001b[38;5;34m60,000\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)                  │          \u001b[38;5;34m30,200\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                   │             \u001b[38;5;34m153\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">271,061</span> (1.03 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m271,061\u001b[0m (1.03 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">90,353</span> (352.94 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m90,353\u001b[0m (352.94 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">180,708</span> (705.89 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m180,708\u001b[0m (705.89 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9a87f428-48d1-4c69-823e-54f5b47dda65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Dimension: 49\n",
      "Output Dimension: 3\n",
      "Epoch 1/50\n",
      "54921/54921 [==============================] - 277s 5ms/step - loss: 0.4840 - accuracy: 0.7433 - val_loss: 0.4842 - val_accuracy: 0.7430\n",
      "Epoch 2/50\n",
      "54921/54921 [==============================] - 276s 5ms/step - loss: 0.4838 - accuracy: 0.7433 - val_loss: 0.4842 - val_accuracy: 0.7430\n",
      "Epoch 3/50\n",
      "54921/54921 [==============================] - 268s 5ms/step - loss: 0.4833 - accuracy: 0.7436 - val_loss: 0.4842 - val_accuracy: 0.7430\n",
      "Epoch 4/50\n",
      "54921/54921 [==============================] - 269s 5ms/step - loss: 0.4832 - accuracy: 0.7437 - val_loss: 0.4840 - val_accuracy: 0.7430\n",
      "Epoch 5/50\n",
      "54921/54921 [==============================] - 268s 5ms/step - loss: 0.4832 - accuracy: 0.7437 - val_loss: 0.4841 - val_accuracy: 0.7430\n",
      "Epoch 6/50\n",
      "54921/54921 [==============================] - 269s 5ms/step - loss: 0.4832 - accuracy: 0.7437 - val_loss: 0.4840 - val_accuracy: 0.7430\n",
      "Epoch 7/50\n",
      "54921/54921 [==============================] - 272s 5ms/step - loss: 0.4832 - accuracy: 0.7437 - val_loss: 0.4851 - val_accuracy: 0.7430\n",
      "Epoch 8/50\n",
      "54921/54921 [==============================] - 269s 5ms/step - loss: 0.4831 - accuracy: 0.7437 - val_loss: 0.4841 - val_accuracy: 0.7430\n",
      "Epoch 9/50\n",
      "54921/54921 [==============================] - 274s 5ms/step - loss: 0.4831 - accuracy: 0.7437 - val_loss: 0.4841 - val_accuracy: 0.7430\n",
      "Epoch 10/50\n",
      "54921/54921 [==============================] - 266s 5ms/step - loss: 0.4830 - accuracy: 0.7437 - val_loss: 0.4854 - val_accuracy: 0.7430\n",
      "Epoch 11/50\n",
      "54921/54921 [==============================] - 268s 5ms/step - loss: 0.4830 - accuracy: 0.7437 - val_loss: 0.4846 - val_accuracy: 0.7430\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " custom_lstm_1 (LSTM)        (None, 5, 100)            60000     \n",
      "                                                                 \n",
      " custom_lstm_2 (LSTM)        (None, 50)                30200     \n",
      "                                                                 \n",
      " custom_dense (Dense)        (None, 3)                 153       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 90353 (352.94 KB)\n",
      "Trainable params: 90353 (352.94 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mnoorche\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Recreate the LSTM model in Sequential format\n",
    "\n",
    "# Get input and output shapes of the model\n",
    "input_dim = lstm_model.input_shape[2]   # Get input dimension \n",
    "output_dim = lstm_model.output_shape[1]  # Get output dimension\n",
    "\n",
    "print(\"Input Dimension:\", input_dim)\n",
    "print(\"Output Dimension:\", output_dim)\n",
    "\n",
    "new_lstm_model = Sequential([\n",
    "    Input(shape=(5, input_dim)),  # Input shape is (timesteps, features)\n",
    "    LSTM(100, return_sequences=True, name='custom_lstm_1'),  # First LSTM layer with 100 units\n",
    "    LSTM(50, name='custom_lstm_2'),  # Second LSTM layer with 50 units\n",
    "    Dense(output_dim, activation='softmax', name='custom_dense')  # Final Dense layer with 3 output units\n",
    "])\n",
    "\n",
    "new_lstm_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "new_lstm_model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                            callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)], \n",
    "                            batch_size=32, \n",
    "                            epochs=50, verbose=1)\n",
    "\n",
    "new_lstm_model.save('LSTM.h5')\n",
    "\n",
    "new_lstm_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "320615fb-8e82-4f50-aca4-4de8e1c991f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17121/17121 [==============================] - 38s 2ms/step\n",
      "Test Accuracy: 0.9915\n",
      "Test Precision: 0.9919\n",
      "Test Recall: 0.9915\n",
      "Test F1 Score: 0.9916\n"
     ]
    }
   ],
   "source": [
    "y_pred = new_lstm_model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_test_classes, y_pred_classes)\n",
    "precision = precision_score(y_test_classes, y_pred_classes, average='weighted')\n",
    "recall = recall_score(y_test_classes, y_pred_classes, average='weighted')\n",
    "f1 = f1_score(y_test_classes, y_pred_classes, average='weighted')\n",
    "\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n",
    "print(f'Test Precision: {precision:.4f}')\n",
    "print(f'Test Recall: {recall:.4f}')\n",
    "print(f'Test F1 Score: {f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6f8a4afb-5af6-4341-b64f-97038ac56147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 - custom_lstm_1\n",
      "Weights shape: (49, 400)\n",
      "Recurrent weights shape: (100, 400)\n",
      "Bias shape: (400,)\n",
      "Layer 1 - custom_lstm_2\n",
      "Weights shape: (100, 200)\n",
      "Recurrent weights shape: (50, 200)\n",
      "Bias shape: (200,)\n",
      "Layer 2 - custom_dense\n",
      "Weights shape: (50, 3)\n",
      "Bias shape: (3,)\n",
      "Saved kernel weights for layer 0 to before_pruning_lstm_layer_0_kernel_weights.csv\n",
      "Saved recurrent weights for layer 0 to before_pruning_lstm_layer_0_recurrent_weights.csv\n",
      "Saved biases for layer 0 to before_pruning_lstm_layer_0_biases.csv\n",
      "Saved kernel weights for layer 1 to before_pruning_lstm_layer_1_kernel_weights.csv\n",
      "Saved recurrent weights for layer 1 to before_pruning_lstm_layer_1_recurrent_weights.csv\n",
      "Saved biases for layer 1 to before_pruning_lstm_layer_1_biases.csv\n",
      "Saved kernel weights for layer 2 to before_pruning_lstm_layer_2_kernel_weights.csv\n",
      "Saved biases for layer 2 to before_pruning_lstm_layer_2_biases.csv\n"
     ]
    }
   ],
   "source": [
    "# Function to save weights to CSV \n",
    "\n",
    "for i, layer in enumerate(new_lstm_model.layers):\n",
    "    weights = layer.get_weights()\n",
    "    if weights:\n",
    "        print(f\"Layer {i} - {layer.name}\")\n",
    "        \n",
    "        # For LSTM layers (which have kernel, recurrent, and bias weights)\n",
    "        if 'lstm' in layer.name:\n",
    "            print(f\"Weights shape: {weights[0].shape}\")  # Kernel weights (input weights)\n",
    "            print(f\"Recurrent weights shape: {weights[1].shape}\")  # Recurrent weights\n",
    "            print(f\"Bias shape: {weights[2].shape}\")  # Bias\n",
    "\n",
    "        # For Dense layers (which only have kernel and bias)\n",
    "        elif 'dense' in layer.name:\n",
    "            print(f\"Weights shape: {weights[0].shape}\")  # Kernel weights (input weights)\n",
    "            print(f\"Bias shape: {weights[1].shape}\")  # Bias\n",
    "\n",
    "\n",
    "# Function to save LSTM weights, recurrent weights, and biases to CSV files\n",
    "def save_lstm_weights_to_csv(model, file_prefix):\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        weights = layer.get_weights()\n",
    "        if weights:  # If the layer has weights (such as LSTM or Dense layers)\n",
    "            \n",
    "            # For LSTM layers (which have kernel, recurrent, and bias weights)\n",
    "            if 'lstm' in layer.name:\n",
    "                # Save kernel weights (input weights)\n",
    "                kernel_df = pd.DataFrame(weights[0])  \n",
    "                kernel_df.to_csv(f\"{file_prefix}_layer_{i}_kernel_weights.csv\", index=False)\n",
    "                print(f\"Saved kernel weights for layer {i} to {file_prefix}_layer_{i}_kernel_weights.csv\")\n",
    "                \n",
    "                # Save recurrent weights\n",
    "                recurrent_df = pd.DataFrame(weights[1])  \n",
    "                recurrent_df.to_csv(f\"{file_prefix}_layer_{i}_recurrent_weights.csv\", index=False)\n",
    "                print(f\"Saved recurrent weights for layer {i} to {file_prefix}_layer_{i}_recurrent_weights.csv\")\n",
    "                \n",
    "                # Save biases\n",
    "                bias_df = pd.DataFrame(weights[2].reshape(1, -1))  \n",
    "                bias_df.to_csv(f\"{file_prefix}_layer_{i}_biases.csv\", index=False)\n",
    "                print(f\"Saved biases for layer {i} to {file_prefix}_layer_{i}_biases.csv\")\n",
    "            \n",
    "            # For Dense layers (which only have kernel weights and biases)\n",
    "            elif 'dense' in layer.name:\n",
    "                # Save kernel weights (input weights)\n",
    "                kernel_df = pd.DataFrame(weights[0])  \n",
    "                kernel_df.to_csv(f\"{file_prefix}_layer_{i}_kernel_weights.csv\", index=False)\n",
    "                print(f\"Saved kernel weights for layer {i} to {file_prefix}_layer_{i}_kernel_weights.csv\")\n",
    "                \n",
    "                # Save biases\n",
    "                bias_df = pd.DataFrame(weights[1].reshape(1, -1))  \n",
    "                bias_df.to_csv(f\"{file_prefix}_layer_{i}_biases.csv\", index=False)\n",
    "                print(f\"Saved biases for layer {i} to {file_prefix}_layer_{i}_biases.csv\")\n",
    "\n",
    "\n",
    "save_lstm_weights_to_csv(new_lstm_model, 'before_pruning_lstm')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4130bce6-348c-418a-ae5f-5ca54a6fa495",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pruning(model, X_train, X_val, X_test, y_train, y_val, y_test, final_sparsity, df_name):\n",
    "    \n",
    "    # dataset_size = the number of samples in the training set.\n",
    "    # batch_size = the number of samples processed in one training step.\n",
    "    # num_epochs = the number of times the entire training set is used to update the model.\n",
    "    \n",
    "    # Parameters for the dataset and training\n",
    "    dataset_size = len(X_train)\n",
    "    batch_size = 32  \n",
    "    num_epochs = 20\n",
    "    \n",
    "    # Pruning parameters as percentages of the total steps\n",
    "    start_pct = 0  # Start pruning at the beginning of the training steps\n",
    "    end_pct = 0.7    # End pruning after 60% of the training steps\n",
    "\n",
    "    # Step calculations\n",
    "    steps_per_epoch = dataset_size / batch_size\n",
    "    total_steps = steps_per_epoch * num_epochs\n",
    "\n",
    "    start_step = int(total_steps * start_pct)\n",
    "    end_step = int(total_steps * end_pct)\n",
    "\n",
    "    # Display the calculated steps\n",
    "    print(f\"Total Steps: {total_steps}\")\n",
    "    print(f\"Start Step: {start_step}\")\n",
    "    print(f\"End Step: {end_step}\")\n",
    "    \n",
    "    start_epoch = start_step // steps_per_epoch\n",
    "    end_epoch = end_step // steps_per_epoch\n",
    "    print(f\"Pruning will start in epoch {int(start_epoch)} and end in epoch {int(end_epoch)}\")\n",
    "\n",
    "    \n",
    "    pruning_params = {\n",
    "        'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.60,\n",
    "                                                                 final_sparsity=final_sparsity,\n",
    "                                                                 begin_step=start_step,\n",
    "                                                                 end_step=end_step,\n",
    "                                                                 power=1,\n",
    "                                                                 frequency=100),\n",
    "    }\n",
    "\n",
    "    pruned_model = tfmot.sparsity.keras.prune_low_magnitude(\n",
    "        model, **pruning_params\n",
    "    )   \n",
    "\n",
    "    \n",
    "    # Compile the pruned model\n",
    "    pruned_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    # Set up pruning callbacks and early stopping\n",
    "    pruning_callbacks = [\n",
    "        tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "        tfmot.sparsity.keras.PruningSummaries(log_dir=tempfile.mkdtemp())\n",
    "    ]\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "      \n",
    "    callbacks = pruning_callbacks + [early_stopping]\n",
    "\n",
    "    history = pruned_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=num_epochs, batch_size=batch_size, callbacks=callbacks)\n",
    "\n",
    "    # Strip pruning wrappers\n",
    "    final_model = tfmot.sparsity.keras.strip_pruning(pruned_model)\n",
    "\n",
    "    # Compile the model again after stripping the pruning wrappers\n",
    "    final_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    final_model.save(f'{df_name}.h5') \n",
    "    \n",
    "    # The number of epochs should be greater than num_epochs_for_end_step to make sure pruning is complete\n",
    "    num_epochs_for_end_step = int(end_step // steps_per_epoch)\n",
    "    the_number_of_epochs = len(history.history['loss'])\n",
    "    # Print the number of epochs and the epoch when pruning finished\n",
    "    print(f\"Number of epochs is: {the_number_of_epochs}, and pruning finished at epoch {num_epochs_for_end_step}\")\n",
    "    \n",
    "    # Check if pruning completed before early stopping\n",
    "    pruning_completed = the_number_of_epochs > num_epochs_for_end_step\n",
    "    print(f\"Pruning completed before early stopping: {pruning_completed}\")\n",
    "\n",
    "    y_pred = final_model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_test_classes, y_pred_classes)\n",
    "    precision = precision_score(y_test_classes, y_pred_classes, average='weighted')\n",
    "    recall = recall_score(y_test_classes, y_pred_classes, average='weighted')\n",
    "    f1 = f1_score(y_test_classes, y_pred_classes, average='weighted')\n",
    "\n",
    "    print(f'Test Accuracy: {accuracy:.4f}')\n",
    "    print(f'Test Precision: {precision:.4f}')\n",
    "    print(f'Test Recall: {recall:.4f}')\n",
    "    print(f'Test F1 Score: {f1:.4f}')\n",
    "    \n",
    "    save_lstm_weights_to_csv(final_model, 'after_pruning_lstm')\n",
    "  \n",
    "    return final_model, pruning_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c5d803be-5d58-4a5d-8075-49645a93df1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Steps: 1098411.875\n",
      "Start Step: 0\n",
      "End Step: 768888\n",
      "Pruning will start in epoch 0 and end in epoch 13\n",
      "Epoch 1/20\n",
      "    1/54921 [..............................] - ETA: 100:18:19 - loss: 0.6007 - accuracy: 0.6250WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0050s vs `on_train_batch_end` time: 0.1584s). Check your callbacks.\n",
      "54921/54921 [==============================] - 333s 6ms/step - loss: 0.0982 - accuracy: 0.9644 - val_loss: 0.0919 - val_accuracy: 0.9686\n",
      "Epoch 2/20\n",
      "54921/54921 [==============================] - 337s 6ms/step - loss: 0.0939 - accuracy: 0.9667 - val_loss: 0.0951 - val_accuracy: 0.9655\n",
      "Epoch 3/20\n",
      "54921/54921 [==============================] - 326s 6ms/step - loss: 0.0952 - accuracy: 0.9654 - val_loss: 0.0935 - val_accuracy: 0.9630\n",
      "Epoch 4/20\n",
      "54921/54921 [==============================] - 322s 6ms/step - loss: 0.0912 - accuracy: 0.9679 - val_loss: 0.0934 - val_accuracy: 0.9677\n",
      "Epoch 5/20\n",
      "54921/54921 [==============================] - 333s 6ms/step - loss: 0.0903 - accuracy: 0.9683 - val_loss: 0.0894 - val_accuracy: 0.9686\n",
      "Epoch 6/20\n",
      "54921/54921 [==============================] - 343s 6ms/step - loss: 0.0897 - accuracy: 0.9684 - val_loss: 0.0884 - val_accuracy: 0.9686\n",
      "Epoch 7/20\n",
      "54921/54921 [==============================] - 321s 6ms/step - loss: 0.0893 - accuracy: 0.9684 - val_loss: 0.0886 - val_accuracy: 0.9687\n",
      "Epoch 8/20\n",
      "54921/54921 [==============================] - 321s 6ms/step - loss: 0.0894 - accuracy: 0.9684 - val_loss: 0.0891 - val_accuracy: 0.9688\n",
      "Epoch 9/20\n",
      "54921/54921 [==============================] - 321s 6ms/step - loss: 0.0890 - accuracy: 0.9685 - val_loss: 0.0881 - val_accuracy: 0.9688\n",
      "Epoch 10/20\n",
      "54921/54921 [==============================] - 338s 6ms/step - loss: 0.0889 - accuracy: 0.9685 - val_loss: 0.0886 - val_accuracy: 0.9688\n",
      "Epoch 11/20\n",
      "54921/54921 [==============================] - 323s 6ms/step - loss: 0.0889 - accuracy: 0.9685 - val_loss: 0.0894 - val_accuracy: 0.9686\n",
      "Epoch 12/20\n",
      "54921/54921 [==============================] - 323s 6ms/step - loss: 0.0888 - accuracy: 0.9685 - val_loss: 0.0886 - val_accuracy: 0.9688\n",
      "Epoch 13/20\n",
      "54921/54921 [==============================] - 326s 6ms/step - loss: 0.0887 - accuracy: 0.9685 - val_loss: 0.0890 - val_accuracy: 0.9688\n",
      "Epoch 14/20\n",
      "54921/54921 [==============================] - 324s 6ms/step - loss: 0.0886 - accuracy: 0.9685 - val_loss: 0.0878 - val_accuracy: 0.9688\n",
      "Epoch 15/20\n",
      "54921/54921 [==============================] - 323s 6ms/step - loss: 0.0886 - accuracy: 0.9685 - val_loss: 0.0887 - val_accuracy: 0.9686\n",
      "Epoch 16/20\n",
      "54921/54921 [==============================] - 318s 6ms/step - loss: 0.0886 - accuracy: 0.9685 - val_loss: 0.0886 - val_accuracy: 0.9688\n",
      "Epoch 17/20\n",
      "54921/54921 [==============================] - 318s 6ms/step - loss: 0.0885 - accuracy: 0.9685 - val_loss: 0.0882 - val_accuracy: 0.9687\n",
      "Epoch 18/20\n",
      "54921/54921 [==============================] - 321s 6ms/step - loss: 0.0885 - accuracy: 0.9685 - val_loss: 0.0883 - val_accuracy: 0.9688\n",
      "Epoch 19/20\n",
      "54921/54921 [==============================] - 341s 6ms/step - loss: 0.0885 - accuracy: 0.9685 - val_loss: 0.0885 - val_accuracy: 0.9688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mnoorche\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of epochs is: 19, and pruning finished at epoch 13\n",
      "Pruning completed before early stopping: True\n",
      "17121/17121 [==============================] - 41s 2ms/step\n",
      "Test Accuracy: 0.9919\n",
      "Test Precision: 0.9923\n",
      "Test Recall: 0.9919\n",
      "Test F1 Score: 0.9920\n",
      "Saved kernel weights for layer 0 to after_pruning_lstm_layer_0_kernel_weights.csv\n",
      "Saved recurrent weights for layer 0 to after_pruning_lstm_layer_0_recurrent_weights.csv\n",
      "Saved biases for layer 0 to after_pruning_lstm_layer_0_biases.csv\n",
      "Saved kernel weights for layer 1 to after_pruning_lstm_layer_1_kernel_weights.csv\n",
      "Saved recurrent weights for layer 1 to after_pruning_lstm_layer_1_recurrent_weights.csv\n",
      "Saved biases for layer 1 to after_pruning_lstm_layer_1_biases.csv\n",
      "Saved kernel weights for layer 2 to after_pruning_lstm_layer_2_kernel_weights.csv\n",
      "Saved biases for layer 2 to after_pruning_lstm_layer_2_biases.csv\n"
     ]
    }
   ],
   "source": [
    "the_lstm_model = load_model('LSTM.h5')\n",
    "PrunedLSTM, pruning_completed = pruning(the_lstm_model, X_train, X_val, X_test, y_train, y_val, y_test, 0.65, 'PrunedLSTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e7efbf1b-5eb4-45be-939c-5937e32f1859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing weights for layer: custom_lstm_1\n",
      "Kernel weights in layer 'custom_lstm_1':\n",
      "  Total number of weights: 19600\n",
      "  Weights changed to zero during pruning: 12740\n",
      "  Percentage of zero weights after pruning: 65.00%\n",
      "\n",
      "Recurrent weights in layer 'custom_lstm_1':\n",
      "  Total number of weights: 40000\n",
      "  Weights changed to zero during pruning: 26000\n",
      "  Percentage of zero weights after pruning: 65.00%\n",
      "\n",
      "Biases in layer 'custom_lstm_1':\n",
      "  Total number of weights: 400\n",
      "  Weights changed to zero during pruning: 0\n",
      "  Percentage of zero weights after pruning: 1.00%\n",
      "\n",
      "Comparing weights for layer: custom_lstm_2\n",
      "Kernel weights in layer 'custom_lstm_2':\n",
      "  Total number of weights: 20000\n",
      "  Weights changed to zero during pruning: 13000\n",
      "  Percentage of zero weights after pruning: 65.00%\n",
      "\n",
      "Recurrent weights in layer 'custom_lstm_2':\n",
      "  Total number of weights: 10000\n",
      "  Weights changed to zero during pruning: 6500\n",
      "  Percentage of zero weights after pruning: 65.00%\n",
      "\n",
      "Biases in layer 'custom_lstm_2':\n",
      "  Total number of weights: 200\n",
      "  Weights changed to zero during pruning: 0\n",
      "  Percentage of zero weights after pruning: 0.00%\n",
      "\n",
      "Comparing weights for layer: custom_dense\n",
      "Weights in layer 'custom_dense':\n",
      "  Total number of weights: 150\n",
      "  Weights changed to zero during pruning: 97\n",
      "  Percentage of zero weights after pruning: 64.67%\n",
      "\n",
      "Biases in layer 'custom_dense':\n",
      "  Total number of weights: 3\n",
      "  Weights changed to zero during pruning: 0\n",
      "  Percentage of zero weights after pruning: 0.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to compare weights before and after pruning, and calculate percentage of zero weights for LSTM\n",
    "def compare_lstm_weights_before_after(model_before, model_after):\n",
    "    for layer in model_before.layers:\n",
    "        # Get the layer name\n",
    "        layer_name = layer.name\n",
    "        \n",
    "        # Get weights and biases from both models\n",
    "        weights_before = model_before.get_layer(layer_name).get_weights()\n",
    "        weights_after = model_after.get_layer(layer_name).get_weights()\n",
    "        \n",
    "        print(f\"Comparing weights for layer: {layer_name}\")\n",
    "        \n",
    "        # Handle LSTM layers (which have kernel, recurrent, and bias weights)\n",
    "        if 'lstm' in layer_name:\n",
    "            # LSTM layers have kernel, recurrent, and biases (some configurations may split biases)\n",
    "            weight_names = ['Kernel weights', 'Recurrent weights', 'Biases']\n",
    "        else:\n",
    "            # Dense layers have just weights and biases\n",
    "            weight_names = ['Weights', 'Biases']\n",
    "\n",
    "        # Iterate through kernel, recurrent, and bias weights for LSTM (or just weights and biases for Dense)\n",
    "        for i, (before, after) in enumerate(zip(weights_before, weights_after)):\n",
    "            weight_type = weight_names[i]  # Get the appropriate name for the weight type\n",
    "            \n",
    "            # Calculate the total number of weights\n",
    "            total_weights = np.prod(before.shape)\n",
    "            \n",
    "            # Calculate the number of weights that have changed to zero\n",
    "            changes_to_zero = (before != 0) & (after == 0)\n",
    "            num_changes_to_zero = np.sum(changes_to_zero)\n",
    "            \n",
    "            # Calculate the number of zero weights after pruning\n",
    "            num_zero_weights_after = np.sum(after == 0)\n",
    "            \n",
    "            # Calculate the percentage of weights that are now zero\n",
    "            percentage_zero_weights = (num_zero_weights_after / total_weights) * 100\n",
    "            \n",
    "            # Print the results\n",
    "            print(f\"{weight_type} in layer '{layer_name}':\")\n",
    "            print(f\"  Total number of weights: {total_weights}\")\n",
    "            print(f\"  Weights changed to zero during pruning: {num_changes_to_zero}\")\n",
    "            print(f\"  Percentage of zero weights after pruning: {percentage_zero_weights:.2f}%\")\n",
    "            print(\"\")\n",
    "\n",
    "# Call the function to compare the original LSTM model and the pruned LSTM model\n",
    "compare_lstm_weights_before_after(new_lstm_model, PrunedLSTM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4a4e1a18-e560-49d4-accc-9e75b1b7a5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_lstm_model = load_model('LSTM.h5')\n",
    "# PrunedLSTM = load_model('PrunedLSTM.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ff286640-775c-4cbe-b5a1-8ef1c3f5c986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4a353042-290c-46fd-933d-30e605a5915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions with overflow protection\n",
    "def sigmoid(x):\n",
    "    x = np.clip(x, -88, 88)  # Clipping to prevent overflow in float32\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))  # Numerical stability\n",
    "    return exp_x / np.sum(exp_x, axis=1)\n",
    "    \n",
    "\n",
    "# Function to perform inference with sparse matrix multiplication for LSTM\n",
    "def sparse_lstm_inference(model, input_data):\n",
    "    timesteps = input_data.shape[1]  # Get the number of timesteps\n",
    "    batch_size = input_data.shape[0]\n",
    "    layer_outputs = input_data\n",
    "\n",
    "    for layer in model.layers:\n",
    "        if 'lstm' in layer.name:  # Only focus on LSTM layers\n",
    "            # Get LSTM weights\n",
    "            lstm_weights = layer.get_weights()\n",
    "            kernel_weights = lstm_weights[0]  # Input weights (kernel)\n",
    "            recurrent_weights = lstm_weights[1]  # Recurrent weights\n",
    "            bias = lstm_weights[2]  # Biases\n",
    "\n",
    "            # Split kernel and recurrent weights for gates: input, forget, cell, output\n",
    "            kernel_i, kernel_f, kernel_c, kernel_o = np.split(kernel_weights, 4, axis=1)\n",
    "            recurrent_i, recurrent_f, recurrent_c, recurrent_o = np.split(recurrent_weights, 4, axis=1)\n",
    "            bias_i, bias_f, bias_c, bias_o = np.split(bias, 4)\n",
    "\n",
    "            # Convert to sparse format for matrix multiplication\n",
    "            kernel_i, kernel_f, kernel_c, kernel_o = csr_matrix(kernel_i), csr_matrix(kernel_f), csr_matrix(kernel_c), csr_matrix(kernel_o)\n",
    "            recurrent_i, recurrent_f, recurrent_c, recurrent_o = csr_matrix(recurrent_i), csr_matrix(recurrent_f), csr_matrix(recurrent_c), csr_matrix(recurrent_o)\n",
    "\n",
    "            # Initialize cell state and hidden state for this LSTM layer based on lstm units in this layer\n",
    "            lstm_units = kernel_i.shape[1]  # The number of LSTM units for this layer\n",
    "\n",
    "            # Initialize cell state and hidden state for each sample in the batch\n",
    "            cell_state = np.zeros((batch_size, lstm_units))  # Initialize cell state (Once per layer, per batch)\n",
    "            hidden_state = np.zeros((batch_size, lstm_units))  # Initialize hidden state (Once per layer, per batch)\n",
    "            cell_state = csr_matrix(cell_state)\n",
    "            hidden_state = csr_matrix(hidden_state)\n",
    "            \n",
    "            if len(layer_outputs.shape) == 3:  # First LSTM layer, process timesteps (3D input)\n",
    "                timestep_hidden_states = np.zeros((batch_size, timesteps, lstm_units))\n",
    "                \n",
    "                for t in range(timesteps):\n",
    "                    current_input = layer_outputs[:, t, :]  # Input for timestep t\n",
    "\n",
    "                    current_input = csr_matrix(current_input)\n",
    "                    \n",
    "                    # Perform matrix multiplications for each gate\n",
    "                    input_gate = sigmoid(current_input @ kernel_i + hidden_state @ recurrent_i + bias_i)\n",
    "                    forget_gate = sigmoid(current_input @ kernel_f + hidden_state @ recurrent_f + bias_f)\n",
    "                    cell_gate = tanh(current_input @ kernel_c + hidden_state @ recurrent_c + bias_c)\n",
    "                    output_gate = sigmoid(current_input @ kernel_o + hidden_state @ recurrent_o + bias_o)\n",
    "\n",
    "                    input_gate = csr_matrix(input_gate)\n",
    "                    forget_gate = csr_matrix(forget_gate)\n",
    "                    cell_gate = csr_matrix(cell_gate)\n",
    "                    output_gate = csr_matrix(output_gate)\n",
    "\n",
    "                    # Update cell state and hidden state\n",
    "                    # cell_state = forget_gate * cell_state + input_gate * cell_gate\n",
    "                    # hidden_state = output_gate * tanh(cell_state)\n",
    "                    cell_state = csr_matrix.multiply(forget_gate, cell_state) + csr_matrix.multiply(input_gate, cell_gate)\n",
    "                    hidden_state = csr_matrix.multiply(output_gate, tanh(cell_state))\n",
    "\n",
    "                    # Store hidden state for this timestep\n",
    "                    timestep_hidden_states[:, t, :] = hidden_state.toarray()\n",
    "\n",
    "                # After processing all timesteps, pass the entire sequence to the next layer\n",
    "                layer_outputs = timestep_hidden_states if layer.return_sequences else hidden_state\n",
    "\n",
    "            else:  # For subsequent LSTM layers, input is 2D (batch_size, lstm_units)\n",
    "                current_input_sparse = csr_matrix(layer_outputs)  # Convert the input to sparse\n",
    "\n",
    "                # Perform matrix multiplications for each gate\n",
    "                input_gate = sigmoid(current_input_sparse @ kernel_i + hidden_state @ recurrent_i + bias_i)\n",
    "                forget_gate = sigmoid(current_input_sparse @ kernel_f + hidden_state @ recurrent_f + bias_f)\n",
    "                cell_gate = tanh(current_input_sparse @ kernel_c + hidden_state @ recurrent_c + bias_c)\n",
    "                output_gate = sigmoid(current_input_sparse @ kernel_o + hidden_state @ recurrent_o + bias_o)\n",
    "\n",
    "                # Update cell state and hidden state\n",
    "                # cell_state = forget_gate * cell_state + input_gate * cell_gate\n",
    "                # hidden_state = output_gate * tanh(cell_state)\n",
    "                cell_state = csr_matrix.multiply(forget_gate, cell_state) + csr_matrix.multiply(input_gate, cell_gate)\n",
    "                hidden_state = csr_matrix.multiply(output_gate, tanh(cell_state))\n",
    "                \n",
    "\n",
    "                # Pass only the last hidden state to the next layer\n",
    "                layer_outputs = hidden_state\n",
    "\n",
    "        elif 'dense' in layer.name:  # Handle dense layers\n",
    "            dense_weights = layer.get_weights()[0]  # Get dense weights\n",
    "            bias = layer.get_weights()[1]  # Get biases\n",
    "\n",
    "            # Convert sparse weights to dense format for multiplication\n",
    "            sparse_weights = csr_matrix(dense_weights)\n",
    "            layer_outputs = csr_matrix(layer_outputs)\n",
    "            \n",
    "            # Perform matrix multiplication (converting sparse weights to dense)\n",
    "            layer_outputs = layer_outputs @ sparse_weights\n",
    "            \n",
    "            # Add bias to each output row\n",
    "            layer_outputs = layer_outputs + bias\n",
    "\n",
    "            # Apply the softmax activation function after the dense layer\n",
    "            if layer.activation.__name__ == 'softmax':\n",
    "                layer_outputs = softmax(layer_outputs)\n",
    "\n",
    "    return layer_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "832407be-02a1-412c-ba26-fcd70a47b168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM sparse inference output: [[1.83942675e-03 3.62657448e-13 9.98160573e-01]]\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Prediction of the original LSTM using predict method: [[1.8394268e-03 3.6265676e-13 9.9816054e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Example input for inference\n",
    "first_sample = X_test[0]\n",
    "first_sample_batch = np.expand_dims(first_sample, axis=0)  # This will make the shape (1, 5, 49)\n",
    "\n",
    "# Perform inference using the LSTM sparse matrix multiplication\n",
    "sparse_inference_output = sparse_lstm_inference(PrunedLSTM, first_sample_batch)\n",
    "print(\"LSTM sparse inference output:\", sparse_inference_output)\n",
    "\n",
    "# Real prediction for comparison\n",
    "predict_original_inference_output = PrunedLSTM.predict(first_sample_batch)\n",
    "print('Prediction of the original LSTM using predict method:', predict_original_inference_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f5872c9b-f061-45d3-b36c-32eefbc58048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_inference_time(model, pruned_model, X_test):\n",
    "    # Lists to store inference times for both models\n",
    "    model_time_list = []\n",
    "    pruned_model_time_list = []\n",
    "\n",
    "    # Model before pruning (original model)\n",
    "    for sample in X_test:\n",
    "        start_time = timeit.default_timer()\n",
    "        model.predict(np.expand_dims(sample, axis=0), verbose=False)\n",
    "        end_time = timeit.default_timer()\n",
    "        model_time_list.append((end_time - start_time) * 1000)  # Convert to milliseconds\n",
    "    \n",
    "    avg_inference_time_model = np.mean(model_time_list)\n",
    "    print(f\"Average inference time per sample for Model before pruning: {avg_inference_time_model:.6f} milliseconds\")\n",
    "\n",
    "    # Model after pruning (pruned model)\n",
    "    for sample in X_test:\n",
    "        start_time = timeit.default_timer()\n",
    "        sparse_lstm_inference(pruned_model, np.expand_dims(sample, axis=0))\n",
    "        end_time = timeit.default_timer()\n",
    "        pruned_model_time_list.append((end_time - start_time) * 1000)  # Convert to milliseconds\n",
    "    \n",
    "    avg_inference_time_pruned_model = np.mean(pruned_model_time_list)\n",
    "    print(f\"Average inference time per sample for Model after pruning: {avg_inference_time_pruned_model:.6f} milliseconds\")\n",
    "   \n",
    "    # Save the lists as NumPy arrays\n",
    "    np.save('model_time_list.npy', np.array(model_time_list))\n",
    "    np.save('pruned_model_time_list.npy', np.array(pruned_model_time_list))\n",
    "    \n",
    "    return avg_inference_time_model, avg_inference_time_pruned_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7be504f6-560b-4885-8159-543245c6acb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsampled X_test shape: (54784, 5, 49)\n"
     ]
    }
   ],
   "source": [
    "sample_size = int(0.1 * X_test.shape[0])  # Calculate 10% of the rows\n",
    "random_sample_set = X_test[np.random.choice(X_test.shape[0], sample_size, replace=False)]\n",
    "print(f\"Subsampled X_test shape: {random_sample_set.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "35e20ec5-6346-4ddb-af85-b856da968cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per sample for Model before pruning: 63.612781 milliseconds\n",
      "Average inference time per sample for Model after pruning: 24.735132 milliseconds\n"
     ]
    }
   ],
   "source": [
    "# Convert DataFrame to NumPy array\n",
    "avg_time_model, avg_time_pruned_model = measure_inference_time(new_lstm_model, PrunedLSTM, random_sample_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3e11ee83-71a3-47ee-830c-35e1f155e98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved original model weights to original_model_weights.npz\n",
      "Saved pruned model weights to pruned_model_weights.npz\n",
      "Size of original_model_weights.npz: 355.07 KB\n",
      "Size of pruned_model_weights.npz: 253.64 KB\n"
     ]
    }
   ],
   "source": [
    "# Function to save all weights of the original model in a single .npz file\n",
    "def save_original_weights_as_npz(model, file_name):\n",
    "    all_weights = {}\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        weights = layer.get_weights()\n",
    "        if weights:\n",
    "            # For LSTM layers (which have kernel, recurrent, and bias weights)\n",
    "            if 'lstm' in layer.name:\n",
    "                all_weights[f\"layer_{i}_kernel_weights\"] = weights[0]  # Kernel weights\n",
    "                all_weights[f\"layer_{i}_recurrent_weights\"] = weights[1]  # Recurrent weights\n",
    "                all_weights[f\"layer_{i}_biases\"] = weights[2]  # Biases\n",
    "            # For Dense layers (which only have kernel weights and biases)\n",
    "            elif 'dense' in layer.name:\n",
    "                all_weights[f\"layer_{i}_weights\"] = weights[0]  # Kernel weights\n",
    "                all_weights[f\"layer_{i}_biases\"] = weights[1]  # Biases\n",
    "\n",
    "    np.savez(file_name, **all_weights)\n",
    "    print(f\"Saved original model weights to {file_name}.npz\")\n",
    "\n",
    "# Function to save all pruned (sparse) weights in a single .npz file\n",
    "def save_pruned_weights_as_npz(model, file_name):\n",
    "    all_sparse_weights = {}\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        weights = layer.get_weights()\n",
    "        if weights:\n",
    "            # For LSTM layers (which have kernel, recurrent, and bias weights)\n",
    "            if 'lstm' in layer.name:\n",
    "                sparse_kernel_weights = csr_matrix(weights[0])  # Convert kernel to sparse\n",
    "                sparse_recurrent_weights = csr_matrix(weights[1])  # Convert recurrent weights to sparse\n",
    "                all_sparse_weights[f\"layer_{i}_sparse_kernel_weights\"] = sparse_kernel_weights\n",
    "                all_sparse_weights[f\"layer_{i}_sparse_recurrent_weights\"] = sparse_recurrent_weights\n",
    "                all_sparse_weights[f\"layer_{i}_biases\"] = weights[2]  # Save biases as dense\n",
    "\n",
    "            # For Dense layers (which only have kernel weights and biases)\n",
    "            elif 'dense' in layer.name:\n",
    "                sparse_weights = csr_matrix(weights[0])  # Convert kernel to sparse\n",
    "                all_sparse_weights[f\"layer_{i}_sparse_weights\"] = sparse_weights\n",
    "                all_sparse_weights[f\"layer_{i}_biases\"] = weights[1]  # Save biases as dense\n",
    "\n",
    "    # Save the sparse weights in .npz format\n",
    "    np.savez(file_name, **all_sparse_weights)\n",
    "    print(f\"Saved pruned model weights to {file_name}.npz\")\n",
    "\n",
    "# Function to compare the sizes of two files\n",
    "def compare_file_sizes(file1, file2):\n",
    "    size1 = os.path.getsize(file1) / 1024  # Convert to KB\n",
    "    size2 = os.path.getsize(file2) / 1024  # Convert to KB\n",
    "    print(f\"Size of {file1}: {size1:.2f} KB\")\n",
    "    print(f\"Size of {file2}: {size2:.2f} KB\")\n",
    "\n",
    "# Save all weights for original and pruned models\n",
    "save_original_weights_as_npz(new_lstm_model, 'original_model_weights')\n",
    "save_pruned_weights_as_npz(PrunedLSTM, 'pruned_model_weights')\n",
    "\n",
    "# Comparing the size of the original and pruned model weight files\n",
    "compare_file_sizes('original_model_weights.npz', 'pruned_model_weights.npz')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
